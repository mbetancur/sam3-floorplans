{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAM3 Fine-tuning for Hospital Floorplan Room Detection\n",
        "\n",
        "This notebook fine-tunes SAM3 to detect room shapes in hospital floorplan images.\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with A100 GPU\n",
        "- Dataset in COCO format mounted on Google Drive\n",
        "\n",
        "**Output:**\n",
        "- Fine-tuned checkpoint for room segmentation\n",
        "- TensorBoard logs for monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone SAM3 repository\n",
        "!git clone https://github.com/facebookresearch/sam3.git\n",
        "%cd sam3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install SAM3 with training dependencies\n",
        "!pip install -e \".[train]\" -q\n",
        "\n",
        "# Install additional dependencies\n",
        "!pip install tensorboard opencv-python-headless -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate with Hugging Face to download model weights\n",
        "from huggingface_hub import login\n",
        "\n",
        "# You need to request access at: https://huggingface.co/facebook/sam3\n",
        "# Then create an access token at: https://huggingface.co/settings/tokens\n",
        "login()  # This will prompt for your token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Configuration\n",
        "\n",
        "Your dataset should be organized as:\n",
        "```\n",
        "drive/MyDrive/datasets/floorplans/\n",
        "├── train/\n",
        "│   ├── _annotations.coco.json\n",
        "│   └── *.png (floorplan images)\n",
        "├── val/\n",
        "│   ├── _annotations.coco.json\n",
        "│   └── *.png\n",
        "└── test/\n",
        "    ├── _annotations.coco.json\n",
        "    └── *.png\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - UPDATE THESE PATHS\n",
        "DATASET_ROOT = \"/content/drive/MyDrive/datasets/floorplans\"\n",
        "EXPERIMENT_DIR = \"/content/drive/MyDrive/sam3_experiments/floorplans\"\n",
        "BPE_PATH = \"assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "\n",
        "# Create experiment directory\n",
        "import os\n",
        "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
        "os.makedirs(f\"{EXPERIMENT_DIR}/checkpoints\", exist_ok=True)\n",
        "os.makedirs(f\"{EXPERIMENT_DIR}/tensorboard\", exist_ok=True)\n",
        "\n",
        "print(f\"Dataset root: {DATASET_ROOT}\")\n",
        "print(f\"Experiment directory: {EXPERIMENT_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify dataset exists\n",
        "import json\n",
        "\n",
        "train_ann_path = f\"{DATASET_ROOT}/train/_annotations.coco.json\"\n",
        "val_ann_path = f\"{DATASET_ROOT}/val/_annotations.coco.json\"\n",
        "\n",
        "def check_dataset(ann_path, split_name):\n",
        "    if not os.path.exists(ann_path):\n",
        "        print(f\"❌ {split_name} annotations not found: {ann_path}\")\n",
        "        return False\n",
        "    \n",
        "    with open(ann_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    num_images = len(data.get('images', []))\n",
        "    num_annotations = len(data.get('annotations', []))\n",
        "    has_segmentation = any('segmentation' in ann for ann in data.get('annotations', []))\n",
        "    \n",
        "    print(f\"✓ {split_name}: {num_images} images, {num_annotations} annotations\")\n",
        "    if not has_segmentation:\n",
        "        print(f\"  ⚠️ WARNING: No segmentation masks found - mask training will fail!\")\n",
        "    return True\n",
        "\n",
        "check_dataset(train_ann_path, \"Train\")\n",
        "check_dataset(val_ann_path, \"Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Start Training\n",
        "\n",
        "The training uses the pre-configured `floorplan_finetune.yaml` config which includes:\n",
        "- Mask loss enabled for precise boundary detection\n",
        "- A100-optimized batch sizes and learning rates\n",
        "- 50 epochs with validation every 5 epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start TensorBoard in the background\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {EXPERIMENT_DIR}/tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training\n",
        "# Update paths in the config before running\n",
        "!sed -i \"s|floorplan_data_root:.*|floorplan_data_root: {DATASET_ROOT}|\" sam3/train/configs/floorplans/floorplan_finetune.yaml\n",
        "!sed -i \"s|experiment_log_dir:.*|experiment_log_dir: {EXPERIMENT_DIR}|\" sam3/train/configs/floorplans/floorplan_finetune.yaml\n",
        "\n",
        "!python sam3/train/train.py \\\n",
        "    -c configs/floorplans/floorplan_finetune.yaml \\\n",
        "    --use-cluster 0 \\\n",
        "    --num-gpus 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Monitor Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List checkpoints\n",
        "import glob\n",
        "\n",
        "checkpoints = glob.glob(f\"{EXPERIMENT_DIR}/checkpoints/*.pt\")\n",
        "checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
        "\n",
        "print(\"Available checkpoints:\")\n",
        "for ckpt in checkpoints[:5]:\n",
        "    size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "    print(f\"  {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Inference with Fine-tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sam3.model_builder import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "# Load the fine-tuned model\n",
        "# Update CHECKPOINT_PATH to your actual checkpoint\n",
        "CHECKPOINT_PATH = f\"{EXPERIMENT_DIR}/checkpoints/checkpoint_final.pt\"\n",
        "\n",
        "model = build_sam3_image_model(\n",
        "    checkpoint_path=CHECKPOINT_PATH,\n",
        "    bpe_path=BPE_PATH,\n",
        "    enable_segmentation=True\n",
        ")\n",
        "processor = Sam3Processor(model)\n",
        "\n",
        "print(\"✓ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_rooms(image, masks, scores, threshold=0.5):\n",
        "    \"\"\"Visualize detected rooms on the floorplan.\"\"\"\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.imshow(image)\n",
        "    \n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
        "    \n",
        "    room_count = 0\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        if score < threshold:\n",
        "            continue\n",
        "        \n",
        "        room_count += 1\n",
        "        color = colors[i % len(colors)]\n",
        "        mask_overlay = np.zeros((*mask.shape, 4))\n",
        "        mask_overlay[mask > 0] = [*color[:3], 0.4]\n",
        "        plt.imshow(mask_overlay)\n",
        "    \n",
        "    plt.title(f\"Detected {room_count} rooms (threshold={threshold})\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test on a sample image\n",
        "test_images = glob.glob(f\"{DATASET_ROOT}/test/*.png\")\n",
        "\n",
        "if test_images:\n",
        "    test_image_path = test_images[0]\n",
        "    image = Image.open(test_image_path)\n",
        "    \n",
        "    # Run inference\n",
        "    inference_state = processor.set_image(image)\n",
        "    output = processor.set_text_prompt(state=inference_state, prompt=\"room\")\n",
        "    \n",
        "    masks = output[\"masks\"]\n",
        "    scores = output[\"scores\"]\n",
        "    \n",
        "    print(f\"Detected {len(masks)} potential rooms\")\n",
        "    print(f\"Score range: {min(scores):.3f} - {max(scores):.3f}\")\n",
        "    \n",
        "    visualize_rooms(image, masks, scores, threshold=0.5)\n",
        "else:\n",
        "    print(\"No test images found. Add images to the test folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Export Model for Deployment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the final checkpoint to a deployment-ready location\n",
        "import shutil\n",
        "\n",
        "final_checkpoint = f\"{EXPERIMENT_DIR}/checkpoints/checkpoint_final.pt\"\n",
        "deployment_path = f\"{EXPERIMENT_DIR}/sam3_floorplan_model.pt\"\n",
        "\n",
        "if os.path.exists(final_checkpoint):\n",
        "    shutil.copy(final_checkpoint, deployment_path)\n",
        "    print(f\"✓ Model exported to: {deployment_path}\")\n",
        "    print(f\"  Size: {os.path.getsize(deployment_path) / (1024*1024):.1f} MB\")\n",
        "else:\n",
        "    # Try to find latest checkpoint\n",
        "    if checkpoints:\n",
        "        latest = checkpoints[0]\n",
        "        shutil.copy(latest, deployment_path)\n",
        "        print(f\"✓ Latest checkpoint exported to: {deployment_path}\")\n",
        "    else:\n",
        "        print(\"No checkpoints found. Run training first.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
